#!/usr/bin/env python3
"""
Generate model card (README.md) for Hugging Face Hub.

Usage:
    python scripts/huggingface/generate_model_card.py \
        --model readability-classifier \
        --type classifier \
        --repo KothaGPT/bn-en-readability-classifier
"""

import argparse
import json
import logging
import sys
from pathlib import Path
from typing import Any, Dict

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


# ==============================
# MODEL CARD TEMPLATES
# ==============================

MODEL_CARD_TEMPLATES = {
    "classifier": (
        "---"
        "\n"
        "language:"
        "\n"
        "- bn"
        "\n"
        "- en"
        "\n"
        "license: apache-2.0"
        "\n"
        "tags:"
        "\n"
        "- bangla"
        "\n"
        "- bengali"
        "\n"
        "- english"
        "\n"
        "- readability"
        "\n"
        "- classifier"
        "\n"
        "- transformers"
        "\n"
        "metrics:"
        "\n"
        "- accuracy"
        "\n"
        "- f1"
        "\n"
        "- precision"
        "\n"
        "- recall"
        "\n"
        "---"
        "\n"
        "\n"
        "# {model_name}"
        "\n"
        "\n"
        "This is a bilingual readability classifier that predicts the difficulty level of Bangla and English text."
        "\n"
        "\n"
        "## üß† Model Description"
        "\n"
        "\n"
        "- **Architecture:** {architecture}"
        "\n"
        "- **Languages:** Bangla, English"
        "\n"
        "- **Task:** Text Readability Classification"
        "\n"
        "- **Labels:** {num_labels} (Simple, Medium, Complex)"
        "\n"
        "- **Framework:** PyTorch + Transformers"
        "\n"
        "- **License:** Apache 2.0"
        "\n"
        "\n"
        "---"
        "\n"
        "\n"
        "## ‚öôÔ∏è Training Configuration"
        "\n"
        "\n"
        "| Parameter | Value |"
        "\n"
        "|------------|--------|"
        "\n"
        "| Base Model | {base_model} |"
        "\n"
        "| Epochs | {epochs} |"
        "\n"
        "| Batch Size | {batch_size} |"
        "\n"
        "| Learning Rate | {learning_rate} |"
        "\n"
        "| Sequence Length | {max_seq_length} |"
        "\n"
        "| Optimizer | {optimizer} |"
        "\n"
        "| Scheduler | {scheduler} |"
        "\n"
        "| Mixed Precision | {mixed_precision} |"
        "\n"
        "| Training Samples | {train_dataset_size:,} |"
        "\n"
        "| Evaluation Samples | {eval_dataset_size:,} |"
        "\n"
        "\n"
        "---"
        "\n"
        "\n"
        "## üß™ Evaluation Results"
        "\n"
        "\n"
        "| Metric | Score |"
        "\n"
        "|--------|--------|"
        "\n"
        "| Accuracy | {accuracy:.3f} |"
        "\n"
        "| F1 (macro) | {f1_macro:.3f} |"
        "\n"
        "| Precision | {precision_macro:.3f} |"
        "\n"
        "| Recall | {recall_macro:.3f} |"
        "\n"
        "\n"
        "**Evaluation Dataset Size:** {eval_samples:,}  "
        "\n"
        "**Framework:** {framework} ({transformers_version})  "
        "\n"
        "**GPU:** {gpu}  "
        "\n"
        "\n"
        "---"
        "\n"
        "\n"
        "## üöÄ Usage Example"
        "\n"
        "\n"
        "```python"
        "\n"
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
        "\n"
        "import torch"
        "\n"
        "\n"
        'model_id = "{repo_id}"'
        "\n"
        "\n"
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
        "\n"
        "model = AutoModelForSequenceClassification.from_pretrained(model_id)"
        "\n"
        "\n"
        'text = "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶¢‡¶æ‡¶ï‡¶æ ‡¶∂‡¶π‡¶∞‡¶ü‡¶ø ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶ï‡ßá‡¶®‡ßç‡¶¶‡ßç‡¶∞‡•§"'
        "\n"
        'inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)'
        "\n"
        "with torch.no_grad():"
        "\n"
        "    logits = model(**inputs).logits"
        "\n"
        "    pred = torch.argmax(logits, dim=-1).item()"
        "\n"
        "\n"
        'labels = ["simple", "medium", "complex"]'
        "\n"
        'print(f"Predicted readability: {{labels[pred]}}")'
        "\n"
        "```"
    )
}
