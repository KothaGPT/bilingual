{
  "benchmark_info": {
    "timestamp": "2025-10-14 23:42:58",
    "system_info": {
      "cpu_count": 8,
      "memory_gb": 16.0,
      "python_version": "3.10.18",
      "cuda_available": false,
      "gpu_count": 0,
      "gpu_name": null
    }
  },
  "models": {
    "models/bilingual-lm/": {
      "model_name": "models/bilingual-lm/",
      "tasks": {
        "generation": {
          "task": "generation",
          "model": "models/bilingual-lm/",
          "max_tokens": 50,
          "batch_performance": {
            "1": {
              "avg_latency_ms": 0.0171661376953125,
              "tokens_per_second": 2912711.111111111,
              "throughput_requests_per_second": 58254.22222222222,
              "successful_generations": 1,
              "memory_used_mb": 0.0
            },
            "4": {
              "avg_latency_ms": 0.010013580322265625,
              "tokens_per_second": 4993219.047619048,
              "throughput_requests_per_second": 399457.5238095238,
              "successful_generations": 4,
              "memory_used_mb": 0.0
            },
            "8": {
              "avg_latency_ms": 0.006943941116333008,
              "tokens_per_second": 7200521.888412017,
              "throughput_requests_per_second": 1152083.5021459227,
              "successful_generations": 8,
              "memory_used_mb": 0.0
            }
          },
          "latency_stats": {
            "mean_latency_ms": 0.011374553044637045,
            "median_latency_ms": 0.010013580322265625,
            "p95_latency_ms": 0.016450881958007812,
            "p99_latency_ms": 0.017023086547851562
          },
          "memory_usage": {},
          "error_rate": 0.0
        }
      },
      "summary": {
        "avg_latency_ms": 0.011374553044637045,
        "total_throughput": 16716247.295319844,
        "avg_memory_usage_mb": 0.0,
        "overall_error_rate": 0.0,
        "tasks_completed": 1
      }
    }
  }
}
