# ğŸŒ Wikipedia Language Model Training

Quick start guide for training language models on Wikipedia data.

---

## ğŸš€ Quick Start

### 1. Download Wikipedia Dump

```bash
python scripts/download_wiki.py --lang bn --output datasets/wikipedia/raw
```

### 2. Preprocess Data

```bash
python scripts/preprocess_wiki.py \
  --input datasets/wikipedia/raw/bnwiki-latest-pages-articles.xml.bz2 \
  --output datasets/wikipedia/processed \
  --lang bn
```

### 3. Train Model

```bash
python scripts/train_wiki_lm.py \
  --data datasets/wikipedia/processed \
  --model ai4bharat/indic-bert \
  --output models/wikipedia/base \
  --epochs 3
```

### 4. Evaluate

```bash
python scripts/evaluate_wiki_lm.py \
  --model models/wikipedia/base \
  --data datasets/wikipedia/processed/test/bn_test.txt
```

### 5. Use the Model

```python
from bilingual.modules.wikipedia_lm import load_model

model = load_model("models/wikipedia/base")
results = model.fill_mask("à¦†à¦®à¦¿ [MASK] à¦–à¦¾à¦‡", top_k=5)

for result in results:
    print(f"{result['sequence']} (score: {result['score']:.4f})")
```

---

## ğŸ“š Available Scripts

| Script | Purpose | Example |
|--------|---------|---------|
| `download_wiki.py` | Download Wikipedia dumps | `python scripts/download_wiki.py --lang bn` |
| `preprocess_wiki.py` | Clean and tokenize text | `python scripts/preprocess_wiki.py --input raw/ --output processed/` |
| `analyze_wiki_dataset.py` | Analyze dataset quality | `python scripts/analyze_wiki_dataset.py --input processed/` |
| `train_wiki_lm.py` | Train language model | `python scripts/train_wiki_lm.py --data processed/ --model ai4bharat/indic-bert` |
| `evaluate_wiki_lm.py` | Evaluate trained model | `python scripts/evaluate_wiki_lm.py --model models/wikipedia/base` |
| `align_bilingual_wiki.py` | Align Bangla-English articles | `python scripts/align_bilingual_wiki.py --bn raw/bn --en raw/en` |

---

## ğŸ¯ Use Cases

### Masked Language Modeling

```python
model = load_model("models/wikipedia/base")

# Fill masked tokens
results = model.fill_mask("à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶à§‡à¦° à¦°à¦¾à¦œà¦§à¦¾à¦¨à§€ [MASK]", top_k=3)
# Output: à¦¢à¦¾à¦•à¦¾, à¦¹à¦šà§à¦›à§‡, à¦›à¦¿à¦²

# Predict next word
predictions = model.predict_next_word("à¦†à¦®à¦¿ à¦­à¦¾à¦¤", top_k=5)
# Output: à¦–à¦¾à¦‡, à¦–à§‡à¦¯à¦¼à§‡à¦›à¦¿, à¦–à¦¾à¦šà§à¦›à¦¿
```

### Text Generation

```python
model = load_model("models/wikipedia/gpt2-bn", model_type='clm')

texts = model.generate_text("à¦†à¦®à¦¿ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼", max_length=100)
# Output: à¦†à¦®à¦¿ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦•à¦¥à¦¾ à¦¬à¦²à¦¿ à¦à¦¬à¦‚ à¦²à¦¿à¦–à¦¿...
```

### Semantic Similarity

```python
similarity = model.compute_similarity(
    "à¦†à¦®à¦¿ à¦­à¦¾à¦¤ à¦–à¦¾à¦‡",
    "à¦†à¦®à¦¿ à¦–à¦¾à¦¬à¦¾à¦° à¦–à¦¾à¦‡"
)
# Output: 0.8532
```

### Embeddings

```python
embedding = model.get_sentence_embedding("à¦†à¦®à¦¿ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦•à¦¥à¦¾ à¦¬à¦²à¦¿")
# Output: torch.Size([768])
```

---

## ğŸ—ï¸ Project Structure

```
datasets/wikipedia/
â”œâ”€â”€ raw/                          # Downloaded dumps
â”‚   â”œâ”€â”€ bnwiki-latest-pages-articles.xml.bz2
â”‚   â””â”€â”€ enwiki-latest-pages-articles.xml.bz2
â”œâ”€â”€ processed/                    # Processed text
â”‚   â”œâ”€â”€ train/bn_train.txt
â”‚   â”œâ”€â”€ val/bn_val.txt
â”‚   â””â”€â”€ test/bn_test.txt
â”œâ”€â”€ bilingual/                    # Aligned Bangla-English
â”‚   â”œâ”€â”€ aligned_articles.json
â”‚   â”œâ”€â”€ bangla.txt
â”‚   â””â”€â”€ english.txt
â””â”€â”€ analysis/                     # Dataset statistics

models/wikipedia/
â”œâ”€â”€ base/                         # Base Wikipedia LM
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ tokenizer_config.json
â””â”€â”€ finetuned_literary/          # Fine-tuned on literary data

scripts/
â”œâ”€â”€ download_wiki.py             # Download Wikipedia dumps
â”œâ”€â”€ preprocess_wiki.py           # Preprocess and clean text
â”œâ”€â”€ analyze_wiki_dataset.py      # Analyze dataset quality
â”œâ”€â”€ train_wiki_lm.py             # Train language model
â”œâ”€â”€ evaluate_wiki_lm.py          # Evaluate model
â””â”€â”€ align_bilingual_wiki.py      # Align bilingual articles

src/bilingual/modules/
â””â”€â”€ wikipedia_lm.py              # Wikipedia LM module
```

---

## ğŸ“– Documentation

- **[Full Training Roadmap](docs/WIKIPEDIA_TRAINING_ROADMAP.md)** - Complete guide with all phases
- **[Usage Examples](docs/examples/wikipedia_lm_usage.md)** - Code examples and recipes
- **[API Documentation](docs/api/index.md)** - API reference

---

## ğŸ”§ Requirements

### Minimum

- Python 3.8+
- 8GB GPU (RTX 2070, T4)
- 16GB RAM
- 50GB disk space

### Recommended

- Python 3.9+
- 16GB+ GPU (V100, A100)
- 32GB+ RAM
- 100GB+ SSD

### Dependencies

```bash
pip install transformers datasets torch accelerate tensorboard
pip install wikiextractor indic-nlp-library
pip install matplotlib numpy scikit-learn
```

Or install all at once:

```bash
pip install -r requirements.txt
```

---

## âš¡ Performance Tips

1. **Use GPU:** Training is 10-50x faster on GPU
2. **Batch Processing:** Process multiple texts at once
3. **FP16 Training:** Enabled by default for 2x speedup
4. **Gradient Accumulation:** Simulate larger batch sizes
5. **Checkpoint Saving:** Save every 1000 steps

---

## ğŸ“ Training Tips

### Start Small

```bash
# Test with 1 epoch first
python scripts/train_wiki_lm.py \
  --data datasets/wikipedia/processed \
  --model ai4bharat/indic-bert \
  --output models/wikipedia/test \
  --epochs 1 \
  --batch-size 8
```

### Production Training

```bash
# Full training with optimizations
python scripts/train_wiki_lm.py \
  --data datasets/wikipedia/processed \
  --model ai4bharat/indic-bert \
  --output models/wikipedia/base \
  --epochs 5 \
  --batch-size 16 \
  --learning-rate 3e-5 \
  --gradient-accumulation-steps 4 \
  --save-steps 1000
```

### Monitor Training

```bash
# Launch TensorBoard
tensorboard --logdir models/wikipedia/base/logs
```

---

## ğŸŒ Bilingual Training

### Download Both Languages

```bash
python scripts/download_wiki.py --bilingual --output datasets/wikipedia/raw
```

### Align Articles

```bash
python scripts/align_bilingual_wiki.py \
  --bn datasets/wikipedia/raw/bn_extracted \
  --en datasets/wikipedia/raw/en_extracted \
  --output datasets/wikipedia/bilingual \
  --use-extracted
```

### Train Cross-lingual Model

```bash
python scripts/train_wiki_lm.py \
  --data datasets/wikipedia/bilingual \
  --model xlm-roberta-base \
  --output models/wikipedia/xlm-bilingual \
  --epochs 3
```

---

## ğŸ› Troubleshooting

### Out of Memory

```bash
# Reduce batch size and increase gradient accumulation
python scripts/train_wiki_lm.py \
  --data datasets/wikipedia/processed \
  --model ai4bharat/indic-bert \
  --output models/wikipedia/base \
  --batch-size 4 \
  --gradient-accumulation-steps 8
```

### Slow Download

```bash
# Use wget with resume capability
wget -c https://dumps.wikimedia.org/bnwiki/latest/bnwiki-latest-pages-articles.xml.bz2
```

### Training Crashes

```bash
# Resume from checkpoint
python scripts/train_wiki_lm.py \
  --data datasets/wikipedia/processed \
  --model models/wikipedia/base/checkpoint-1000 \
  --output models/wikipedia/base \
  --epochs 3
```

---

## ğŸ“Š Expected Results

| Metric | Value | Notes |
|--------|-------|-------|
| **Perplexity** | 20-40 | On Bangla Wikipedia test set |
| **Training Time** | 1-3 days | Single V100 GPU |
| **Model Size** | ~400MB | BERT-base |
| **Vocabulary** | 50K-100K | Bangla tokens |

---

## ğŸ¤ Contributing

Contributions welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- **AI4Bharat** for Indic BERT models
- **Wikimedia Foundation** for Wikipedia dumps
- **HuggingFace** for Transformers library
- **Indic NLP Library** for text processing

---

## ğŸ“¬ Contact

For questions or issues, please open an issue on GitHub or contact the maintainers.

---

## ğŸ”— Links

- [GitHub Repository](https://github.com/KothaGPT/bilingual)
- [Documentation](docs/WIKIPEDIA_TRAINING_ROADMAP.md)
- [Examples](docs/examples/wikipedia_lm_usage.md)
- [API Reference](docs/api/index.md)
